{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\Codebasics\\NLP_Projects\\spam_fraud_message\\data\\raw\\spam_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8650bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Dataset {len(df)}\")\n",
    "print(f\"Total Spam {(df['label_binary']== 1).sum()}\")\n",
    "print(f\"Total Ham {(df['label_binary']==0).sum()}\")\n",
    "\n",
    "\n",
    "ham_df = df[df['label_binary']==0]\n",
    "spam_df = df[df['label_binary']==1]\n",
    "\n",
    "\n",
    "total_class_imbalance = (len(spam_df)/len(ham_df))*100\n",
    "\n",
    "total_class_imbalance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df['label_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,     # IMPORTANT\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8474c",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── VECTORIZATION ────────────────────────────────────────────────────────────\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)  # learn vocab + transform train\n",
    "X_test_tfidf  = vectorizer.transform(X_test)        # only transform test (no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── TRAIN MODEL ──────────────────────────────────────────────────────────────\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ── PREDICTIONS ──────────────────────────────────────────────────────────────\n",
    "y_pred       = model_nb.predict(X_test_tfidf)         # hard class labels (0 or 1)\n",
    "y_pred_proba = model_nb.predict_proba(X_test_tfidf)[:, 1]  # probability of being spam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ── CORE METRICS ─────────────────────────────────────────────────────────────\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# ACCURACY: % of ALL emails classified correctly (ham + spam)\n",
    "# e.g. 0.97 → 97 out of 100 emails correctly labeled\n",
    "# ⚠ Misleading on imbalanced data (model can ignore spam and still score high)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "# PRECISION: Of all emails the model CALLED spam, how many actually were spam?\n",
    "# e.g. 0.95 → when model says \"spam\", it's right 95% of the time\n",
    "# High precision = few false alarms (ham flagged as spam)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# RECALL (Sensitivity): Of all ACTUAL spam emails, how many did the model catch?\n",
    "# e.g. 0.89 → model catches 89% of real spam, misses 11%\n",
    "# High recall = few spam emails slip through to inbox\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "# F1 SCORE: Harmonic mean of Precision and Recall\n",
    "# Balances both — useful when you care about both false alarms AND missed spam\n",
    "# e.g. 0.92 → good balance between precision and recall\n",
    "# Range: 0 (worst) → 1 (best)\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "# ROC-AUC: Model's ability to distinguish spam vs ham across ALL thresholds\n",
    "# e.g. 0.98 → model separates spam/ham with 98% reliability\n",
    "# 0.5 = random guessing, 1.0 = perfect classifier\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c818974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ── CONFUSION MATRIX ─────────────────────────────────────────────────────────\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#                  Predicted Ham   Predicted Spam\n",
    "# Actual Ham    [[  TN (correct) ,  FP (false alarm) ]]\n",
    "# Actual Spam   [[  FN (missed)  ,  TP (caught spam) ]]\n",
    "#\n",
    "# TN = True Negative:  Ham correctly identified as ham\n",
    "# FP = False Positive: Ham wrongly flagged as spam  ← annoying for user\n",
    "# FN = False Negative: Spam missed, lands in inbox  ← dangerous\n",
    "# TP = True Positive:  Spam correctly caught\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives  (ham   → ham)   : {TN}\")\n",
    "print(f\"  False Positives (ham   → spam)  : {FP}  ← legit emails wrongly blocked\")\n",
    "print(f\"  False Negatives (spam  → ham)   : {FN}  ← spam that slipped through\")\n",
    "print(f\"  True Positives  (spam  → spam)  : {TP}\")\n",
    "\n",
    "# ── FULL CLASSIFICATION REPORT ───────────────────────────────────────────────\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
    "# Shows Precision, Recall, F1 broken down per class\n",
    "# 'support' = number of actual samples in each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── VISUALIZATIONS ───────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1 — Confusion Matrix Heatmap\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "    xticklabels=[\"Ham\", \"Spam\"],\n",
    "    yticklabels=[\"Ham\", \"Spam\"]\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "axes[0].set_ylabel(\"Actual Label\")\n",
    "axes[0].set_title(\"Confusion Matrix\\n(bigger diagonal = better)\")\n",
    "\n",
    "# Plot 2 — ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "# fpr = False Positive Rate (ham wrongly called spam at each threshold)\n",
    "# tpr = True Positive Rate  (spam correctly caught at each threshold)\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label=\"Random Guess (AUC = 0.5)\")\n",
    "axes[1].set_xlabel(\"False Positive Rate (ham flagged as spam)\")\n",
    "axes[1].set_ylabel(\"True Positive Rate (spam caught)\")\n",
    "axes[1].set_title(\"ROC Curve\\n(closer to top-left = better)\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"evaluation_plots.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ── SUMMARY ──────────────────────────────────────────────────────────────────\n",
    "print(\"\\n── METRIC SUMMARY ──────────────────────────────────────────\")\n",
    "print(f\"  Accuracy  {accuracy:.4f}  → overall correct classifications\")\n",
    "print(f\"  Precision {precision:.4f}  → how trustworthy 'spam' predictions are\")\n",
    "print(f\"  Recall    {recall:.4f}  → how much actual spam is caught\")\n",
    "print(f\"  F1        {f1:.4f}  → balance of precision & recall\")\n",
    "print(f\"  ROC-AUC   {roc_auc:.4f}  → overall discrimination ability\")\n",
    "print(\"────────────────────────────────────────────────────────────\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b31cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ── TRAIN XGBoost ─────────────────────────────────────────────────────────────\n",
    "# scale_pos_weight: compensates for class imbalance\n",
    "# = count(ham) / count(spam)  →  tells XGBoost to penalize missing spam more\n",
    "# e.g. if 90% ham / 10% spam → scale_pos_weight = 9\n",
    "model_xgb = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # handles imbalanced dataset\n",
    "    eval_metric=\"logloss\"               # log loss: measures probability calibration\n",
    ")\n",
    "model_xgb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ── PREDICTIONS ───────────────────────────────────────────────────────────────\n",
    "y_pred_xgb       = model_xgb.predict(X_test_tfidf)\n",
    "y_pred_proba_xgb = model_xgb.predict_proba(X_test_tfidf)[:, 1]  # spam probability\n",
    "\n",
    "# ── METRICS ───────────────────────────────────────────────────────────────────\n",
    "accuracy_xgb  = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb    = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb        = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc_xgb   = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "cm_xgb        = confusion_matrix(y_test, y_pred_xgb)\n",
    "TN_x, FP_x, FN_x, TP_x = cm_xgb.ravel()\n",
    "\n",
    "print(\"── XGBoost Evaluation ───────────────────────────────────────\")\n",
    "print(f\"  Accuracy  {accuracy_xgb:.4f}  → overall correct classifications\")\n",
    "print(f\"  Precision {precision_xgb:.4f}  → how trustworthy 'spam' predictions are\")\n",
    "print(f\"  Recall    {recall_xgb:.4f}  → how much actual spam is caught\")\n",
    "print(f\"  F1        {f1_xgb:.4f}  → balance of precision & recall\")\n",
    "print(f\"  ROC-AUC   {roc_auc_xgb:.4f}  → overall spam vs ham separation\")\n",
    "print(f\"\\n  True Negatives  (ham  → ham)  : {TN_x}\")\n",
    "print(f\"  False Positives (ham  → spam) : {FP_x}  ← legit emails wrongly blocked\")\n",
    "print(f\"  False Negatives (spam → ham)  : {FN_x}  ← spam that slipped through\")\n",
    "print(f\"  True Positives  (spam → spam) : {TP_x}\")\n",
    "print(\"─────────────────────────────────────────────────────────────\")\n",
    "\n",
    "print(\"\\nClassification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=[\"Ham\", \"Spam\"]))\n",
    "\n",
    "# ── MODEL COMPARISON TABLE ────────────────────────────────────────────────────\n",
    "# Reuse NB metrics computed earlier (model_nb evaluation must have run first)\n",
    "y_pred_nb       = model_nb.predict(X_test_tfidf)\n",
    "y_pred_proba_nb = model_nb.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"],\n",
    "    \"Naive Bayes\": [\n",
    "        accuracy_score(y_test, y_pred_nb),\n",
    "        precision_score(y_test, y_pred_nb),\n",
    "        recall_score(y_test, y_pred_nb),\n",
    "        f1_score(y_test, y_pred_nb),\n",
    "        roc_auc_score(y_test, y_pred_proba_nb)\n",
    "    ],\n",
    "    \"XGBoost\": [\n",
    "        accuracy_xgb, precision_xgb,\n",
    "        recall_xgb, f1_xgb, roc_auc_xgb\n",
    "    ]\n",
    "})\n",
    "comparison[\"Winner\"] = comparison.apply(\n",
    "    lambda row: \"XGBoost ✓\" if row[\"XGBoost\"] > row[\"Naive Bayes\"] else \"Naive Bayes ✓\",\n",
    "    axis=1\n",
    ")\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# ── VISUALIZATIONS ────────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(\"XGBoost Evaluation & Model Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# ── Plot 1: XGBoost Confusion Matrix ─────────────────────────────────────────\n",
    "sns.heatmap(\n",
    "    cm_xgb, annot=True, fmt='d', cmap='Oranges', ax=axes[0],\n",
    "    xticklabels=[\"Ham\", \"Spam\"],\n",
    "    yticklabels=[\"Ham\", \"Spam\"]\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "axes[0].set_ylabel(\"Actual Label\")\n",
    "axes[0].set_title(\"XGBoost — Confusion Matrix\\n(bigger diagonal = better)\")\n",
    "\n",
    "# ── Plot 2: ROC Curve Comparison ──────────────────────────────────────────────\n",
    "# XGBoost ROC\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "axes[1].plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2,\n",
    "             label=f\"XGBoost   (AUC = {roc_auc_xgb:.4f})\")\n",
    "\n",
    "# Naive Bayes ROC (overlaid for comparison)\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_pred_proba_nb)\n",
    "axes[1].plot(fpr_nb, tpr_nb, color='steelblue', lw=2, linestyle='--',\n",
    "             label=f\"Naive Bayes (AUC = {roc_auc_score(y_test, y_pred_proba_nb):.4f})\")\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], color='grey', lw=1, linestyle=':', label=\"Random Guess\")\n",
    "axes[1].set_xlabel(\"False Positive Rate (ham flagged as spam)\")\n",
    "axes[1].set_ylabel(\"True Positive Rate (spam caught)\")\n",
    "axes[1].set_title(\"ROC Curve — NB vs XGBoost\\n(closer to top-left = better)\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# ── Plot 3: Metric Bar Chart Comparison ───────────────────────────────────────\n",
    "metrics     = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "nb_scores   = comparison[\"Naive Bayes\"].values\n",
    "xgb_scores  = comparison[\"XGBoost\"].values\n",
    "x           = np.arange(len(metrics))\n",
    "width       = 0.35\n",
    "\n",
    "bars_nb  = axes[2].bar(x - width/2, nb_scores,  width, label=\"Naive Bayes\", color=\"steelblue\",  alpha=0.85)\n",
    "bars_xgb = axes[2].bar(x + width/2, xgb_scores, width, label=\"XGBoost\",    color=\"darkorange\", alpha=0.85)\n",
    "\n",
    "# Annotate bar values\n",
    "for bar in bars_nb:\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f\"{bar.get_height():.2f}\", ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars_xgb:\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f\"{bar.get_height():.2f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(metrics, rotation=15)\n",
    "axes[2].set_ylim(0, 1.12)\n",
    "axes[2].set_ylabel(\"Score\")\n",
    "axes[2].set_title(\"Metric Comparison\\nNaive Bayes vs XGBoost\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"xgb_vs_nb_evaluation.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4824092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model_svm = LinearSVC(class_weight=\"balanced\")\n",
    "model_svm.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# ── TRAIN SVM ─────────────────────────────────────────────────────────────────\n",
    "# class_weight=\"balanced\": automatically adjusts weights inversely proportional\n",
    "# to class frequency → same effect as scale_pos_weight in XGBoost\n",
    "# e.g. if spam is 10% of data, spam samples get 9x more weight during training\n",
    "# LinearSVC: linear kernel SVM — fast and effective for high-dim text (TF-IDF)\n",
    "model_svm = LinearSVC(class_weight=\"balanced\")\n",
    "model_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ── PROBABILITY CALIBRATION ───────────────────────────────────────────────────\n",
    "# ⚠ LinearSVC does NOT output probabilities natively (only decision scores)\n",
    "# CalibratedClassifierCV wraps it to produce proper probabilities via cross-val\n",
    "# method=\"sigmoid\" fits a Platt scaling layer on top of the SVM decision scores\n",
    "# Needed for: ROC-AUC with probabilities, threshold tuning, confidence scores\n",
    "model_svm_cal = CalibratedClassifierCV(model_svm, method=\"sigmoid\", cv=5)\n",
    "model_svm_cal.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ── PREDICTIONS ───────────────────────────────────────────────────────────────\n",
    "y_pred_svm       = model_svm.predict(X_test_tfidf)            # hard labels (0 or 1)\n",
    "y_pred_proba_svm = model_svm_cal.predict_proba(X_test_tfidf)[:, 1]  # spam probability\n",
    "\n",
    "# ── METRICS ───────────────────────────────────────────────────────────────────\n",
    "accuracy_svm  = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm    = recall_score(y_test, y_pred_svm)\n",
    "f1_svm        = f1_score(y_test, y_pred_svm)\n",
    "roc_auc_svm   = roc_auc_score(y_test, y_pred_proba_svm)\n",
    "cm_svm        = confusion_matrix(y_test, y_pred_svm)\n",
    "TN_s, FP_s, FN_s, TP_s = cm_svm.ravel()\n",
    "\n",
    "print(\"── LinearSVC Evaluation ─────────────────────────────────────\")\n",
    "print(f\"  Accuracy  {accuracy_svm:.4f}  → overall correct classifications\")\n",
    "print(f\"  Precision {precision_svm:.4f}  → how trustworthy 'spam' predictions are\")\n",
    "print(f\"  Recall    {recall_svm:.4f}  → how much actual spam is caught\")\n",
    "print(f\"  F1        {f1_svm:.4f}  → balance of precision & recall\")\n",
    "print(f\"  ROC-AUC   {roc_auc_svm:.4f}  → overall spam vs ham separation\")\n",
    "print(f\"\\n  True Negatives  (ham  → ham)  : {TN_s}\")\n",
    "print(f\"  False Positives (ham  → spam) : {FP_s}  ← legit emails wrongly blocked\")\n",
    "print(f\"  False Negatives (spam → ham)  : {FN_s}  ← spam that slipped through\")\n",
    "print(f\"  True Positives  (spam → spam) : {TP_s}\")\n",
    "print(\"─────────────────────────────────────────────────────────────\")\n",
    "\n",
    "print(\"\\nClassification Report (LinearSVC):\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=[\"Ham\", \"Spam\"]))\n",
    "\n",
    "# ── 3-MODEL COMPARISON TABLE ──────────────────────────────────────────────────\n",
    "y_pred_nb        = model_nb.predict(X_test_tfidf)\n",
    "y_pred_proba_nb  = model_nb.predict_proba(X_test_tfidf)[:, 1]\n",
    "y_pred_xgb       = model_xgb.predict(X_test_tfidf)\n",
    "y_pred_proba_xgb = model_xgb.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "scores = {\n",
    "    \"Naive Bayes\": [\n",
    "        accuracy_score(y_test, y_pred_nb),\n",
    "        precision_score(y_test, y_pred_nb),\n",
    "        recall_score(y_test, y_pred_nb),\n",
    "        f1_score(y_test, y_pred_nb),\n",
    "        roc_auc_score(y_test, y_pred_proba_nb)\n",
    "    ],\n",
    "    \"XGBoost\": [\n",
    "        accuracy_score(y_test, y_pred_xgb),\n",
    "        precision_score(y_test, y_pred_xgb),\n",
    "        recall_score(y_test, y_pred_xgb),\n",
    "        f1_score(y_test, y_pred_xgb),\n",
    "        roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "    ],\n",
    "    \"LinearSVC\": [\n",
    "        accuracy_svm, precision_svm,\n",
    "        recall_svm, f1_svm, roc_auc_svm\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(scores, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"])\n",
    "\n",
    "# Mark the best model per metric with ✓\n",
    "def highlight_winner(row):\n",
    "    best = row.idxmax()\n",
    "    return [f\"{v:.4f} ✓\" if col == best else f\"{v:.4f}\" for col, v in row.items()]\n",
    "\n",
    "comparison_display = comparison.apply(highlight_winner, axis=1, result_type=\"expand\")\n",
    "comparison_display.columns = comparison.columns\n",
    "print(\"\\n── 3-Model Comparison ───────────────────────────────────────\")\n",
    "print(comparison_display.to_string())\n",
    "print(\"─────────────────────────────────────────────────────────────\")\n",
    "\n",
    "# ── VISUALIZATIONS ────────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(\"LinearSVC Evaluation & 3-Model Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# ── Plot 1: SVM Confusion Matrix ──────────────────────────────────────────────\n",
    "sns.heatmap(\n",
    "    cm_svm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "    xticklabels=[\"Ham\", \"Spam\"],\n",
    "    yticklabels=[\"Ham\", \"Spam\"]\n",
    ")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "axes[0].set_ylabel(\"Actual Label\")\n",
    "axes[0].set_title(\"LinearSVC — Confusion Matrix\\n(bigger diagonal = better)\")\n",
    "\n",
    "# ── Plot 2: ROC Curve — All 3 Models ─────────────────────────────────────────\n",
    "fpr_nb,  tpr_nb,  _ = roc_curve(y_test, y_pred_proba_nb)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_pred_proba_svm)\n",
    "\n",
    "axes[1].plot(fpr_svm, tpr_svm, color='seagreen',   lw=2,\n",
    "             label=f\"LinearSVC   (AUC = {roc_auc_svm:.4f})\")\n",
    "axes[1].plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2, linestyle='--',\n",
    "             label=f\"XGBoost     (AUC = {roc_auc_score(y_test, y_pred_proba_xgb):.4f})\")\n",
    "axes[1].plot(fpr_nb,  tpr_nb,  color='steelblue',  lw=2, linestyle=':',\n",
    "             label=f\"Naive Bayes (AUC = {roc_auc_score(y_test, y_pred_proba_nb):.4f})\")\n",
    "axes[1].plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--', label=\"Random Guess\")\n",
    "axes[1].set_xlabel(\"False Positive Rate (ham flagged as spam)\")\n",
    "axes[1].set_ylabel(\"True Positive Rate (spam caught)\")\n",
    "axes[1].set_title(\"ROC Curve — All 3 Models\\n(closer to top-left = better)\")\n",
    "axes[1].legend(loc=\"lower right\", fontsize=8)\n",
    "\n",
    "# ── Plot 3: Grouped Bar Chart — All 3 Models ─────────────────────────────────\n",
    "metrics    = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "nb_scores  = comparison[\"Naive Bayes\"].values\n",
    "xgb_scores = comparison[\"XGBoost\"].values\n",
    "svm_scores = comparison[\"LinearSVC\"].values\n",
    "x          = np.arange(len(metrics))\n",
    "width      = 0.25\n",
    "\n",
    "bars_nb  = axes[2].bar(x - width, nb_scores,  width, label=\"Naive Bayes\", color=\"steelblue\",  alpha=0.85)\n",
    "bars_xgb = axes[2].bar(x,         xgb_scores, width, label=\"XGBoost\",    color=\"darkorange\", alpha=0.85)\n",
    "bars_svm = axes[2].bar(x + width, svm_scores, width, label=\"LinearSVC\",  color=\"seagreen\",   alpha=0.85)\n",
    "\n",
    "for bars in [bars_nb, bars_xgb, bars_svm]:\n",
    "    for bar in bars:\n",
    "        axes[2].text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.004,\n",
    "            f\"{bar.get_height():.2f}\",\n",
    "            ha='center', va='bottom', fontsize=7\n",
    "        )\n",
    "\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(metrics, rotation=15)\n",
    "axes[2].set_ylim(0, 1.15)\n",
    "axes[2].set_ylabel(\"Score\")\n",
    "axes[2].set_title(\"All Metrics — 3 Model Comparison\")\n",
    "axes[2].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"svm_3model_evaluation.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7a12556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available : False\n",
      "Device         : CPU ← this is your problem\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='366' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [366/366 39:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.991627</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>0.955357</td>\n",
       "      <td>0.968326</td>\n",
       "      <td>0.995782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Test Set Results ─────────────────────────────────────────\n",
      "  eval_loss                      0.0813\n",
      "  eval_accuracy                  0.9916\n",
      "  eval_precision                 0.9730\n",
      "  eval_recall                    0.9643\n",
      "  eval_f1                        0.9686\n",
      "  eval_roc_auc                   0.9983\n",
      "  eval_runtime                   42.3698\n",
      "  eval_samples_per_second        19.7310\n",
      "  eval_steps_per_second          0.3300\n",
      "  epoch                          3.0000\n",
      "─────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOPTION 1 — Google Colab (free GPU)\\n  • Go to colab.research.google.com\\n  • Runtime → Change runtime type → T4 GPU\\n  • Paste this code → runs in ~10-15 mins instead of 2.5hrs\\n\\nOPTION 2 — Kaggle Notebooks (free GPU, 30hr/week)\\n  • kaggle.com → New Notebook → Settings → Accelerator → GPU T4 x2\\n\\nOPTION 3 — Skip BERT, use the lighter models you already have\\n  • Your LinearSVC / XGBoost with TF-IDF already runs in seconds\\n  • For spam detection specifically, they often match BERT performance\\n  • Only worth using BERT if those models are underperforming\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 0 — CHECK WHAT HARDWARE YOU HAVE\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "print(f\"CUDA available : {torch.cuda.is_available()}\")\n",
    "print(f\"Device         : {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU ← this is your problem'}\")\n",
    "print(f\"GPU Memory     : {round(torch.cuda.get_device_properties(0).total_memory/1e9, 1)} GB\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "# If CUDA is not available, you have 3 options (see bottom of file)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# SPEED FIX 1 — USE A LIGHTER MODEL (biggest speedup if no GPU)\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# bert-base-uncased    → 110M params, SLOW on CPU (~2.5hrs as you saw)\n",
    "# distilbert-base      →  66M params, 40% faster, ~97% of BERT accuracy ✓\n",
    "# bert-tiny            →   4M params, 10x faster, good for quick experiments\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"   # ← swap here, rest of code unchanged\n",
    "# MODEL_NAME = \"bert-base-uncased\"       # ← use this if you have a GPU\n",
    "\n",
    "# FIX: BertTokenizer does NOT work with DistilBERT — use AutoTokenizer instead\n",
    "# AutoTokenizer automatically picks the correct tokenizer for any model name\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# use_fast=True → uses Rust-based tokenizer, ~10x faster than pure Python\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 1 — TOKENIZATION\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,    # SPEED FIX 2: reduced 128 → 64\n",
    "                          # spam emails are short, 128 tokens is overkill\n",
    "                          # cuts sequence processing time by ~50%\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(X_train)\n",
    "val_encodings   = tokenize(X_val)\n",
    "test_encodings  = tokenize(X_test)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 2 — DATASET\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "class SpamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels    = np.array(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SpamDataset(train_encodings, y_train)\n",
    "val_dataset   = SpamDataset(val_encodings,   y_val)\n",
    "test_dataset  = SpamDataset(test_encodings,  y_test)\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 3 — MODEL\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ").to(device)   # SPEED FIX 3: explicitly move model to GPU if available\n",
    "\n",
    "pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits\n",
    "        weights = torch.tensor([1.0, pos_weight], dtype=torch.float).to(logits.device)\n",
    "        loss    = torch.nn.CrossEntropyLoss(weight=weights)(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 4 — TRAINING ARGUMENTS (speed-tuned)\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "training_args = TrainingArguments(\n",
    "    output_dir    = \"./results\",\n",
    "    num_train_epochs              = 3,\n",
    "\n",
    "    # SPEED FIX 4: larger batch = fewer steps = faster epoch\n",
    "    # 16 → 32 if GPU has ≥8GB VRAM, keep 16 for CPU/small GPU\n",
    "    per_device_train_batch_size   = 32,\n",
    "    per_device_eval_batch_size    = 64,   # eval doesn't need gradients, can be bigger\n",
    "\n",
    "    learning_rate                 = 2e-5,\n",
    "\n",
    "    # SPEED FIX 5: evaluate every 200 steps instead of every epoch\n",
    "    # avoids running full val set evaluation too frequently\n",
    "    eval_strategy                 = \"steps\",\n",
    "    eval_steps                    = 200,\n",
    "    save_strategy                 = \"steps\",\n",
    "    save_steps                    = 200,\n",
    "\n",
    "    load_best_model_at_end        = True,\n",
    "    metric_for_best_model         = \"f1\",\n",
    "    greater_is_better             = True,\n",
    "\n",
    "    # SPEED FIX 6: mixed precision — uses float16 instead of float32\n",
    "    # ~2x speedup and ~50% less GPU memory (only works on GPU)\n",
    "    fp16                          = torch.cuda.is_available(),\n",
    "\n",
    "    # SPEED FIX 7: dataloader workers\n",
    "    # ⚠ Windows CRASH FIX: must be 0 on Windows — multiprocessing spawn\n",
    "    # method on Windows causes \"DataLoader worker exited unexpectedly\"\n",
    "    # 0 = load data in main process (safe on Windows, negligible speed loss)\n",
    "    # Only set > 0 on Linux/Mac or inside if __name__ == \"__main__\" guard\n",
    "    dataloader_num_workers        = 0,   # ← was 2, crashes on Windows\n",
    "\n",
    "    # SPEED FIX 8: gradient accumulation — simulates larger batch on small GPU\n",
    "    # effective batch = per_device_train_batch_size × gradient_accumulation_steps\n",
    "    gradient_accumulation_steps   = 1,   # increase to 2 or 4 if GPU OOM errors\n",
    "\n",
    "    logging_steps                 = 50,\n",
    "    logging_dir                   = \"./logs\",\n",
    "    report_to                     = \"none\",   # disables wandb/tensorboard noise\n",
    ")\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 5 — METRICS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds          = np.argmax(logits, axis=1)\n",
    "    proba          = softmax(logits, axis=1)[:, 1]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, proba)\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"roc_auc\": auc}\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 6 — TRAIN & EVALUATE\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "trainer = WeightedTrainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_dataset,\n",
    "    eval_dataset    = val_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"\\n── Test Set Results ─────────────────────────────────────────\")\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"  {metric:<30} {value:.4f}\")\n",
    "print(\"─────────────────────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "344ed1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./final_spam_model\\\\tokenizer_config.json',\n",
       " './final_spam_model\\\\special_tokens_map.json',\n",
       " './final_spam_model\\\\vocab.txt',\n",
       " './final_spam_model\\\\added_tokens.json',\n",
       " './final_spam_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH = \"./final_spam_model\"\n",
    "\n",
    "trainer.save_model(SAVE_PATH)        # saves model + config\n",
    "tokenizer.save_pretrained(SAVE_PATH) # saves tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0fb1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
